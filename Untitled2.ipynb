{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e662fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "from zemberek import (\n",
    "    TurkishSpellChecker,\n",
    "    TurkishSentenceNormalizer,\n",
    "    TurkishSentenceExtractor,\n",
    "    TurkishMorphology,\n",
    "    TurkishTokenizer\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "from zemberek import TurkishMorphology, TurkishSentenceNormalizer\n",
    "\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from openpyxl import Workbook\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5efdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"veriseti.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_indices = df['normalized_text'].isnull()\n",
    "print(df[nan_indices])\n",
    "df = df.dropna(subset=['normalized_text'])\n",
    "print(df[nan_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cca1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['normalized_text'] = df['normalized_text'].str.lower()\n",
    "df['normalized_text'] = df['normalized_text'].astype(str)\n",
    "df['normalized_text'] = df['normalized_text'].apply(lambda x: re.sub('[^a-zA-ZğüşıöçĞÜŞİÖÇ]', ' ', x))\n",
    "df['normalized_text'] = df['normalized_text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "df['normalized_text'] = df['normalized_text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8875e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "morphology = TurkishMorphology.create_with_defaults()\n",
    "normalizer = TurkishSentenceNormalizer(morphology)\n",
    "df['normalized_text'] = df['normalized_text'].apply(lambda x: normalizer.normalize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0464328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "turkce_stopwords = set(stopwords.words('turkish'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = ' '.join(word for word in text.split() if word not in turkce_stopwords)\n",
    "    return text\n",
    "\n",
    "df['normalized_text'] = df['normalized_text'].apply(remove_stopwords)\n",
    "\n",
    "comment_words = ''\n",
    "stopwords = set(stopwords.words('english')).union(turkce_stopwords)\n",
    "\n",
    "for val in df.normalized_text:\n",
    "    val = str(val)\n",
    "    tokens = val.split()\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "    \n",
    "    comment_words\n",
    "\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba82c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk import ngrams\n",
    "\n",
    "def get_ngrams(text, n):\n",
    "    n_grams = ngrams(nltk.word_tokenize(text), n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "df['unigrams'] = df['normalized_text'].apply(lambda x: get_ngrams(x, 1))\n",
    "df['bigrams'] = df['normalized_text'].apply(lambda x: get_ngrams(x, 2))\n",
    "df['trigrams'] = df['normalized_text'].apply(lambda x: get_ngrams(x, 3))\n",
    "\n",
    "\n",
    "def plot_ngrams(df, column, title):\n",
    "    freq_dict = {}\n",
    "    for row in df[column]:\n",
    "        for ngram in row:\n",
    "            if ngram in freq_dict:\n",
    "                freq_dict[ngram] += 1\n",
    "            else:\n",
    "                freq_dict[ngram] = 1\n",
    "    freq_df = pd.DataFrame(list(freq_dict.items()), columns=['ngram', 'frequency'])\n",
    "    freq_df = freq_df.sort_values(by='frequency', ascending=False).reset_index(drop=True)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.barplot(x='ngram', y='frequency', data=freq_df.head(20))\n",
    "    ax.set(title=title)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "plot_ngrams(df, 'unigrams', 'Top 20 unigrams')\n",
    "plot_ngrams(df, 'bigrams', 'Top 20 bigrams')\n",
    "plot_ngrams(df, 'trigrams', 'Top 20 trigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825cd6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "turkce_stopwords = set(stopwords.words('turkish'))\n",
    "\n",
    "additional_stopwords = [\"bir\", \"i\"]\n",
    "\n",
    "# Remove stopwords function\n",
    "def remove_stopwords(text, stopwords):\n",
    "    text = ' '.join([\"fiyat\" if word == \"f\" else \"performans\" if word == \"p\" else word for word in text.split() if word not in stopwords])\n",
    "    return text\n",
    "\n",
    "# Additional stopwords'ları da kullanarak stopword'leri kaldıralım\n",
    "df['normalized_text'] = df['normalized_text'].apply(lambda x: remove_stopwords(x, turkce_stopwords.union(additional_stopwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02309f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf_vect.fit_transform(df['normalized_text'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vect.get_feature_names())\n",
    "\n",
    "result_df = pd.concat([df['Rating'], tfidf_df], axis=1)\n",
    "\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b675d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, log_loss, matthews_corrcoef\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "X = result_df.drop('Rating', axis=1)\n",
    "y = result_df['Rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(),\n",
    "    LGBMClassifier(),\n",
    "    GradientBoostingClassifier()\n",
    "]\n",
    "\n",
    "predictions_df = pd.DataFrame({'Rating': y_test})\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    predictions_df[model.__class__.__name__] = y_pred\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(model.__class__.__name__, \"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(model.__class__.__name__, \"Total Accuracy:\", accuracy)\n",
    "\n",
    "    unique_classes = y_train.unique()\n",
    "    for unique_class in unique_classes:\n",
    "        accuracy = accuracy_score(y_test[y_test == unique_class], y_pred[y_test == unique_class])\n",
    "        print(model.__class__.__name__, \"Accuracy_\", unique_class, \":\", accuracy)\n",
    "    \n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    logloss = log_loss(y_test, y_pred_proba)\n",
    "    print(model.__class__.__name__, \"Log Loss:\", logloss)\n",
    "    \n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    print(model.__class__.__name__, \"MCC:\", mcc)\n",
    "    \n",
    "    print(\"------------------------------\")\n",
    "\n",
    "result_df = pd.concat([result_df, predictions_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e246010",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "\n",
    "def predict_rating(yorum):\n",
    "    # Yorumu TF-IDF dönüşümüne tabi tutma\n",
    "    yorum_tfidf = tfidf_vect.transform([yorum])\n",
    "\n",
    "    # Modelle tahmin yapma\n",
    "    y_pred = model.predict(yorum_tfidf)[0]\n",
    "\n",
    "    # Tahmin edilen sınıfı bir yıldız olarak göster\n",
    "    ratings = {\n",
    "        1: \"⭐️\",\n",
    "        2: \"⭐️⭐️\",\n",
    "        3: \"⭐️⭐️⭐️\",\n",
    "        4: \"⭐️⭐️⭐️⭐️\",\n",
    "        5: \"⭐️⭐️⭐️⭐️⭐️\"\n",
    "    }\n",
    "\n",
    "    rating = ratings[y_pred]\n",
    "\n",
    "    return rating\n",
    "\n",
    "# Gradio arayüzünü oluşturma\n",
    "inputs = gr.inputs.Textbox(label=\"Yorum\")\n",
    "outputs = gr.outputs.Textbox(label=\"Tahmin Edilen Puan\")\n",
    "interface = gr.Interface(fn=predict_rating, inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Arayüzü başlatma\n",
    "interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605f366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f4248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77d1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
